<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Research Results - Faber</title>
  <meta name="description" content="LLM learnability research from faber-trials">
  <link rel="stylesheet" href="/styles.css">
  <link rel="alternate" type="text/plain" href="/llms.txt" title="LLM Site Index">
  <link rel="alternate" type="text/markdown" href="/faber-complete.md" title="Complete Documentation (Markdown)">
</head>
<body>
  <div class="banner">Pre-alpha: Published for early evaluation only</div>
  <div class="container">
    <nav class="site-nav">
  <a href="/" class="nav-home">Faber</a>
  <div class="nav-section">docs</div>
  <a href="/docs/index.html">Documentation</a>
  <a href="/docs/grammar.html">Grammar Reference</a>
  <a href="/docs/examples.html">Examples</a>
  <a href="/docs/examples-full.html">All Examples</a>
  <a href="/docs/fundamenta.html">Fundamenta</a>
  <a href="/docs/typi.html">Typi</a>
  <a href="/docs/operatores.html">Operatores</a>
  <a href="/docs/structurae.html">Structurae</a>
  <a href="/docs/regimen.html">Regimen</a>
  <a href="/docs/functiones.html">Functiones</a>
  <a href="/docs/importa.html">Importa</a>
  <a href="/docs/errores.html">Errores</a>
  <a href="/docs/preamble.html">Faber Romanus Grammar</a>
  <div class="nav-section">research</div>
  <a href="/research/index.html">Research</a>
  <a href="/research/results.html" class="active">Research Results</a>
  <div class="nav-subheadings">
    <a href="/research/results.html#model-comparison-cost-vs-speed-vs-accuracy">Model Comparison: Cost vs Speed vs Accuracy</a>
    <a href="/research/results.html#three-level-grading-breakdown">Three-Level Grading Breakdown</a>
    <a href="/research/results.html#by-context-level">By Context Level</a>
    <a href="/research/results.html#by-n-shot-learning-curve">By N-shot (Learning Curve)</a>
    <a href="/research/results.html#error-distribution">Error Distribution</a>
    <a href="/research/results.html#by-task">By Task</a>
    <a href="/research/results.html#methodology">Methodology</a>
  </div>
  <a href="/research/framework-1.1.html">Framework 1.1 Results</a>
</nav>
    <main class="content">
      <h1 id="research-results">Research Results</h1>
<p>Results from the faber-trials evaluation harness. Testing whether LLMs can learn Faber syntax from examples.</p>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Value</th>
</tr>
</thead>
<tbody><tr>
<td>Framework version</td>
<td>1.1</td>
</tr>
<tr>
<td>Total evaluations</td>
<td>12,696</td>
</tr>
<tr>
<td>Models tested</td>
<td>17</td>
</tr>
<tr>
<td>Total cost</td>
<td>$11.50</td>
</tr>
<tr>
<td>Total tokens</td>
<td>9.0M in / 520K out</td>
</tr>
<tr>
<td>Total time</td>
<td>18088.3s</td>
</tr>
</tbody></table>
<h2 id="model-comparison-cost-vs-speed-vs-accuracy">Model Comparison: Cost vs Speed vs Accuracy</h2>
<table>
<thead>
<tr>
<th>Model</th>
<th>Accuracy</th>
<th>Avg Latency</th>
<th>Cost</th>
<th>Tokens</th>
</tr>
</thead>
<tbody><tr>
<td>mistral-7b</td>
<td>100%</td>
<td>1.0s</td>
<td>&lt;$0.01</td>
<td>10K</td>
</tr>
<tr>
<td>gpt-4o</td>
<td>91%</td>
<td>711ms</td>
<td>$1.53</td>
<td>576K</td>
</tr>
<tr>
<td>qwen3-coder</td>
<td>90%</td>
<td>1.4s</td>
<td>$0.20</td>
<td>842K</td>
</tr>
<tr>
<td>gpt-5</td>
<td>89%</td>
<td>6.7s</td>
<td>$4.37</td>
<td>584K</td>
</tr>
<tr>
<td>gpt-4o-mini</td>
<td>88%</td>
<td>814ms</td>
<td>$0.09</td>
<td>576K</td>
</tr>
<tr>
<td>claude-3.5-sonnet</td>
<td>88%</td>
<td>1.8s</td>
<td>$2.21</td>
<td>667K</td>
</tr>
<tr>
<td>gpt-3.5-turbo</td>
<td>88%</td>
<td>490ms</td>
<td>$0.38</td>
<td>721K</td>
</tr>
<tr>
<td>llama-3.1-70b</td>
<td>86%</td>
<td>1.1s</td>
<td>$0.21</td>
<td>584K</td>
</tr>
<tr>
<td>codestral</td>
<td>86%</td>
<td>416ms</td>
<td>$0.18</td>
<td>580K</td>
</tr>
<tr>
<td>deepseek-v3.1</td>
<td>85%</td>
<td>2.0s</td>
<td>$0.09</td>
<td>582K</td>
</tr>
<tr>
<td>claude-4.5-sonnet</td>
<td>77%</td>
<td>1.5s</td>
<td>$1.74</td>
<td>518K</td>
</tr>
<tr>
<td>mercury-coder</td>
<td>73%</td>
<td>589ms</td>
<td>$0.22</td>
<td>834K</td>
</tr>
<tr>
<td>llama-3.1-8b</td>
<td>73%</td>
<td>875ms</td>
<td>$0.03</td>
<td>663K</td>
</tr>
<tr>
<td>claude-3-haiku</td>
<td>69%</td>
<td>950ms</td>
<td>$0.21</td>
<td>728K</td>
</tr>
<tr>
<td>llama-3.2-3b</td>
<td>55%</td>
<td>2.8s</td>
<td>&lt;$0.01</td>
<td>26K</td>
</tr>
<tr>
<td>llama-3.2-1b</td>
<td>16%</td>
<td>480ms</td>
<td>$0.03</td>
<td>749K</td>
</tr>
<tr>
<td>qwen2.5-coder-32b</td>
<td>0%</td>
<td>7.2s</td>
<td>$0.02</td>
<td>253K</td>
</tr>
</tbody></table>
<h2 id="three-level-grading-breakdown">Three-Level Grading Breakdown</h2>
<p><strong>A</strong> = typechecks, <strong>B</strong> = runs without error, <strong>C</strong> = correct output.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Tests</th>
<th>A (Typechecks)</th>
<th>B (Runs)</th>
<th>C (Correct)</th>
</tr>
</thead>
<tbody><tr>
<td>mistral-7b</td>
<td>18</td>
<td>100%</td>
<td>100%</td>
<td>100%</td>
</tr>
<tr>
<td>gpt-4o</td>
<td>840</td>
<td>95%</td>
<td>95%</td>
<td>91%</td>
</tr>
<tr>
<td>qwen3-coder</td>
<td>1008</td>
<td>95%</td>
<td>95%</td>
<td>90%</td>
</tr>
<tr>
<td>gpt-5</td>
<td>672</td>
<td>93%</td>
<td>93%</td>
<td>89%</td>
</tr>
<tr>
<td>gpt-4o-mini</td>
<td>840</td>
<td>93%</td>
<td>93%</td>
<td>88%</td>
</tr>
<tr>
<td>claude-3.5-sonnet</td>
<td>840</td>
<td>95%</td>
<td>95%</td>
<td>88%</td>
</tr>
<tr>
<td>gpt-3.5-turbo</td>
<td>1133</td>
<td>91%</td>
<td>91%</td>
<td>88%</td>
</tr>
<tr>
<td>llama-3.1-70b</td>
<td>840</td>
<td>92%</td>
<td>92%</td>
<td>86%</td>
</tr>
<tr>
<td>codestral</td>
<td>840</td>
<td>93%</td>
<td>93%</td>
<td>86%</td>
</tr>
<tr>
<td>deepseek-v3.1</td>
<td>840</td>
<td>94%</td>
<td>94%</td>
<td>85%</td>
</tr>
<tr>
<td>claude-4.5-sonnet</td>
<td>672</td>
<td>93%</td>
<td>93%</td>
<td>77%</td>
</tr>
<tr>
<td>mercury-coder</td>
<td>840</td>
<td>76%</td>
<td>76%</td>
<td>73%</td>
</tr>
<tr>
<td>llama-3.1-8b</td>
<td>1008</td>
<td>90%</td>
<td>90%</td>
<td>73%</td>
</tr>
<tr>
<td>claude-3-haiku</td>
<td>924</td>
<td>92%</td>
<td>92%</td>
<td>69%</td>
</tr>
<tr>
<td>llama-3.2-3b</td>
<td>22</td>
<td>64%</td>
<td>64%</td>
<td>55%</td>
</tr>
<tr>
<td>llama-3.2-1b</td>
<td>1077</td>
<td>44%</td>
<td>44%</td>
<td>16%</td>
</tr>
<tr>
<td>qwen2.5-coder-32b</td>
<td>282</td>
<td>29%</td>
<td>29%</td>
<td>0%</td>
</tr>
</tbody></table>
<h2 id="by-context-level">By Context Level</h2>
<p>How much documentation context helps models learn Faber.</p>
<table>
<thead>
<tr>
<th>Context</th>
<th>Tests</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody><tr>
<td>examples-only</td>
<td>2604</td>
<td>61%</td>
</tr>
<tr>
<td>grammar-only</td>
<td>2226</td>
<td>82%</td>
</tr>
<tr>
<td>minimal</td>
<td>2924</td>
<td>78%</td>
</tr>
<tr>
<td>basic</td>
<td>2466</td>
<td>79%</td>
</tr>
<tr>
<td>complete</td>
<td>2476</td>
<td>79%</td>
</tr>
</tbody></table>
<h2 id="by-n-shot-learning-curve">By N-shot (Learning Curve)</h2>
<p>Effect of few-shot examples on accuracy.</p>
<table>
<thead>
<tr>
<th>Examples</th>
<th>Tests</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody><tr>
<td>0-shot</td>
<td>3343</td>
<td>70%</td>
</tr>
<tr>
<td>1-shot</td>
<td>3012</td>
<td>72%</td>
</tr>
<tr>
<td>3-shot</td>
<td>3401</td>
<td>80%</td>
</tr>
<tr>
<td>10-shot</td>
<td>2940</td>
<td>81%</td>
</tr>
</tbody></table>
<h2 id="error-distribution">Error Distribution</h2>
<p>Where failures occur (among failed trials only).</p>
<table>
<thead>
<tr>
<th>Error Type</th>
<th>Count</th>
<th>% of Failures</th>
</tr>
</thead>
<tbody><tr>
<td>wrong_output</td>
<td>1330</td>
<td>43%</td>
</tr>
<tr>
<td>type_error</td>
<td>1248</td>
<td>40%</td>
</tr>
<tr>
<td>no_response</td>
<td>312</td>
<td>10%</td>
</tr>
<tr>
<td>syntax_error</td>
<td>192</td>
<td>6%</td>
</tr>
<tr>
<td>runtime_error</td>
<td>13</td>
<td>0%</td>
</tr>
</tbody></table>
<h2 id="by-task">By Task</h2>
<table>
<thead>
<tr>
<th>Task</th>
<th>Tests</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody><tr>
<td>faber_to_ts_functio_string</td>
<td>304</td>
<td>95%</td>
</tr>
<tr>
<td>faber_to_ts_arithmetic</td>
<td>302</td>
<td>94%</td>
</tr>
<tr>
<td>faber_to_ts_ex_pro</td>
<td>303</td>
<td>93%</td>
</tr>
<tr>
<td>faber_to_ts_si_true</td>
<td>304</td>
<td>92%</td>
</tr>
<tr>
<td>faber_to_ts_functio</td>
<td>304</td>
<td>92%</td>
</tr>
<tr>
<td>faber_to_ts_fixum</td>
<td>303</td>
<td>91%</td>
</tr>
<tr>
<td>faber_to_ts_string</td>
<td>304</td>
<td>91%</td>
</tr>
<tr>
<td>faber_to_ts_si_false</td>
<td>304</td>
<td>91%</td>
</tr>
<tr>
<td>faber_to_ts_varia</td>
<td>304</td>
<td>90%</td>
</tr>
<tr>
<td>faber_to_ts_dum</td>
<td>302</td>
<td>90%</td>
</tr>
<tr>
<td>predict_const_value</td>
<td>302</td>
<td>87%</td>
</tr>
<tr>
<td>faber_to_ts_boolean</td>
<td>302</td>
<td>85%</td>
</tr>
<tr>
<td>ts_to_faber_const</td>
<td>304</td>
<td>82%</td>
</tr>
<tr>
<td>complete_const_keyword</td>
<td>301</td>
<td>82%</td>
</tr>
<tr>
<td>ts_to_faber_string</td>
<td>304</td>
<td>81%</td>
</tr>
<tr>
<td>ts_to_faber_arithmetic</td>
<td>303</td>
<td>80%</td>
</tr>
<tr>
<td>complete_return_keyword</td>
<td>301</td>
<td>79%</td>
</tr>
<tr>
<td>complete_let_keyword</td>
<td>301</td>
<td>79%</td>
</tr>
<tr>
<td>complete_function_keyword</td>
<td>300</td>
<td>79%</td>
</tr>
<tr>
<td>complete_while_keyword</td>
<td>300</td>
<td>79%</td>
</tr>
<tr>
<td>ts_to_faber_let</td>
<td>303</td>
<td>78%</td>
</tr>
<tr>
<td>predict_simple_output</td>
<td>302</td>
<td>77%</td>
</tr>
<tr>
<td>complete_print_keyword</td>
<td>299</td>
<td>77%</td>
</tr>
<tr>
<td>ts_to_faber_if_false</td>
<td>304</td>
<td>76%</td>
</tr>
<tr>
<td>predict_arithmetic_parens</td>
<td>301</td>
<td>76%</td>
</tr>
<tr>
<td>predict_function_math</td>
<td>301</td>
<td>76%</td>
</tr>
<tr>
<td>ts_to_faber_if_true</td>
<td>304</td>
<td>75%</td>
</tr>
<tr>
<td>ts_to_faber_while</td>
<td>303</td>
<td>75%</td>
</tr>
<tr>
<td>predict_loop_sum</td>
<td>301</td>
<td>75%</td>
</tr>
<tr>
<td>complete_else_keyword</td>
<td>300</td>
<td>75%</td>
</tr>
<tr>
<td>predict_conditional_true</td>
<td>302</td>
<td>73%</td>
</tr>
<tr>
<td>complete_loop_keyword</td>
<td>301</td>
<td>72%</td>
</tr>
<tr>
<td>predict_conditional_false</td>
<td>303</td>
<td>71%</td>
</tr>
<tr>
<td>predict_arithmetic_precedence</td>
<td>302</td>
<td>68%</td>
</tr>
<tr>
<td>ts_to_faber_for_of</td>
<td>304</td>
<td>67%</td>
</tr>
<tr>
<td>predict_loop_output</td>
<td>301</td>
<td>65%</td>
</tr>
<tr>
<td>predict_function_call</td>
<td>301</td>
<td>65%</td>
</tr>
<tr>
<td>ts_to_faber_function</td>
<td>304</td>
<td>62%</td>
</tr>
<tr>
<td>ts_to_faber_boolean</td>
<td>303</td>
<td>56%</td>
</tr>
<tr>
<td>ts_to_faber_function_string</td>
<td>304</td>
<td>55%</td>
</tr>
<tr>
<td>predict_boolean_and</td>
<td>301</td>
<td>16%</td>
</tr>
<tr>
<td>predict_boolean_or</td>
<td>300</td>
<td>13%</td>
</tr>
</tbody></table>
<h2 id="methodology">Methodology</h2>
<ul>
<li><strong>Temperature:</strong> 0.0 (deterministic)</li>
<li><strong>Seed:</strong> 42 (reproducible)</li>
<li><strong>Dialects:</strong> Latin keywords</li>
<li><strong>Context levels:</strong> examples-only, minimal, basic, complete</li>
</ul>
<p>See <a href="https://github.com/ianzepp/faber-trials">faber-trials</a> for raw data and methodology details.</p>

    </main>
  </div>
</body>
</html>
