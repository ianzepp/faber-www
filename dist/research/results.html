<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Research Results - Faber</title>
  <meta name="description" content="LLM learnability research from faber-trials">
  <link rel="stylesheet" href="/styles.css">
  <link rel="alternate" type="text/plain" href="/llms.txt" title="LLM Site Index">
  <link rel="alternate" type="text/markdown" href="/faber-complete.md" title="Complete Documentation (Markdown)">
</head>
<body>
  <div class="banner">Pre-alpha: Published for early evaluation only</div>
  <div class="container">
    <nav class="site-nav">
  <a href="/" class="nav-home">Faber</a>
  <div class="nav-section">docs</div>
  <a href="/docs/index.html">Documentation</a>
  <a href="/docs/grammar.html">Grammar Reference</a>
  <a href="/docs/examples.html">Examples</a>
  <a href="/docs/examples-full.html">All Examples</a>
  <a href="/docs/fundamenta.html">Fundamenta</a>
  <a href="/docs/typi.html">Typi</a>
  <a href="/docs/operatores.html">Operatores</a>
  <a href="/docs/structurae.html">Structurae</a>
  <a href="/docs/regimen.html">Regimen</a>
  <a href="/docs/functiones.html">Functiones</a>
  <a href="/docs/importa.html">Importa</a>
  <a href="/docs/errores.html">Errores</a>
  <a href="/docs/preamble.html">Faber Romanus Grammar</a>
  <div class="nav-section">research</div>
  <a href="/research/index.html">Research</a>
  <a href="/research/results.html" class="active">Research Results</a>
  <div class="nav-subheadings">
    <a href="/research/results.html#model-comparison-cost-vs-speed-vs-accuracy">Model Comparison: Cost vs Speed vs Accuracy</a>
    <a href="/research/results.html#three-level-grading-breakdown">Three-Level Grading Breakdown</a>
    <a href="/research/results.html#by-context-level">By Context Level</a>
    <a href="/research/results.html#by-n-shot-learning-curve">By N-shot (Learning Curve)</a>
    <a href="/research/results.html#error-distribution">Error Distribution</a>
    <a href="/research/results.html#by-task">By Task</a>
    <a href="/research/results.html#methodology">Methodology</a>
  </div>
</nav>
    <main class="content">
      <h1 id="research-results">Research Results</h1>
<p>Results from the faber-trials evaluation harness. Testing whether LLMs can learn Faber syntax from examples.</p>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Value</th>
</tr>
</thead>
<tbody><tr>
<td>Framework version</td>
<td>1.1</td>
</tr>
<tr>
<td>Total evaluations</td>
<td>10,017</td>
</tr>
<tr>
<td>Models tested</td>
<td>17</td>
</tr>
<tr>
<td>Total cost</td>
<td>$8.24</td>
</tr>
<tr>
<td>Total tokens</td>
<td>6.7M in / 412K out</td>
</tr>
<tr>
<td>Total time</td>
<td>14322.1s</td>
</tr>
</tbody></table>
<h2 id="model-comparison-cost-vs-speed-vs-accuracy">Model Comparison: Cost vs Speed vs Accuracy</h2>
<table>
<thead>
<tr>
<th>Model</th>
<th>Accuracy</th>
<th>Avg Latency</th>
<th>Cost</th>
<th>Tokens</th>
</tr>
</thead>
<tbody><tr>
<td>mistral-7b</td>
<td>100%</td>
<td>1.0s</td>
<td>&lt;$0.01</td>
<td>10K</td>
</tr>
<tr>
<td>gpt-4o</td>
<td>90%</td>
<td>732ms</td>
<td>$1.20</td>
<td>449K</td>
</tr>
<tr>
<td>gpt-3.5-turbo</td>
<td>89%</td>
<td>489ms</td>
<td>$0.26</td>
<td>494K</td>
</tr>
<tr>
<td>qwen3-coder</td>
<td>88%</td>
<td>1.2s</td>
<td>$0.11</td>
<td>452K</td>
</tr>
<tr>
<td>claude-3.5-sonnet</td>
<td>87%</td>
<td>1.9s</td>
<td>$1.72</td>
<td>517K</td>
</tr>
<tr>
<td>gpt-4o-mini</td>
<td>86%</td>
<td>840ms</td>
<td>$0.07</td>
<td>449K</td>
</tr>
<tr>
<td>llama-3.1-70b</td>
<td>85%</td>
<td>1.1s</td>
<td>$0.16</td>
<td>456K</td>
</tr>
<tr>
<td>codestral</td>
<td>85%</td>
<td>417ms</td>
<td>$0.14</td>
<td>449K</td>
</tr>
<tr>
<td>gpt-5</td>
<td>85%</td>
<td>7.5s</td>
<td>$2.38</td>
<td>279K</td>
</tr>
<tr>
<td>deepseek-v3.1</td>
<td>84%</td>
<td>2.2s</td>
<td>$0.07</td>
<td>448K</td>
</tr>
<tr>
<td>claude-4.5-sonnet</td>
<td>77%</td>
<td>1.5s</td>
<td>$1.74</td>
<td>518K</td>
</tr>
<tr>
<td>llama-3.1-8b</td>
<td>73%</td>
<td>872ms</td>
<td>$0.03</td>
<td>535K</td>
</tr>
<tr>
<td>mercury-coder</td>
<td>71%</td>
<td>588ms</td>
<td>$0.17</td>
<td>655K</td>
</tr>
<tr>
<td>claude-3-haiku</td>
<td>69%</td>
<td>960ms</td>
<td>$0.17</td>
<td>576K</td>
</tr>
<tr>
<td>llama-3.2-3b</td>
<td>55%</td>
<td>2.8s</td>
<td>&lt;$0.01</td>
<td>26K</td>
</tr>
<tr>
<td>llama-3.2-1b</td>
<td>16%</td>
<td>450ms</td>
<td>$0.02</td>
<td>606K</td>
</tr>
<tr>
<td>qwen2.5-coder-32b</td>
<td>0%</td>
<td>7.3s</td>
<td>$0.02</td>
<td>243K</td>
</tr>
</tbody></table>
<h2 id="three-level-grading-breakdown">Three-Level Grading Breakdown</h2>
<p><strong>A</strong> = typechecks, <strong>B</strong> = runs without error, <strong>C</strong> = correct output.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Tests</th>
<th>A (Typechecks)</th>
<th>B (Runs)</th>
<th>C (Correct)</th>
</tr>
</thead>
<tbody><tr>
<td>mistral-7b</td>
<td>18</td>
<td>100%</td>
<td>100%</td>
<td>100%</td>
</tr>
<tr>
<td>gpt-4o</td>
<td>672</td>
<td>94%</td>
<td>94%</td>
<td>90%</td>
</tr>
<tr>
<td>gpt-3.5-turbo</td>
<td>757</td>
<td>92%</td>
<td>91%</td>
<td>89%</td>
</tr>
<tr>
<td>qwen3-coder</td>
<td>672</td>
<td>93%</td>
<td>93%</td>
<td>88%</td>
</tr>
<tr>
<td>claude-3.5-sonnet</td>
<td>672</td>
<td>94%</td>
<td>94%</td>
<td>87%</td>
</tr>
<tr>
<td>gpt-4o-mini</td>
<td>672</td>
<td>92%</td>
<td>92%</td>
<td>86%</td>
</tr>
<tr>
<td>llama-3.1-70b</td>
<td>672</td>
<td>91%</td>
<td>91%</td>
<td>85%</td>
</tr>
<tr>
<td>codestral</td>
<td>672</td>
<td>92%</td>
<td>92%</td>
<td>85%</td>
</tr>
<tr>
<td>gpt-5</td>
<td>396</td>
<td>89%</td>
<td>89%</td>
<td>85%</td>
</tr>
<tr>
<td>deepseek-v3.1</td>
<td>672</td>
<td>94%</td>
<td>93%</td>
<td>84%</td>
</tr>
<tr>
<td>claude-4.5-sonnet</td>
<td>672</td>
<td>93%</td>
<td>93%</td>
<td>77%</td>
</tr>
<tr>
<td>llama-3.1-8b</td>
<td>840</td>
<td>90%</td>
<td>90%</td>
<td>73%</td>
</tr>
<tr>
<td>mercury-coder</td>
<td>672</td>
<td>74%</td>
<td>74%</td>
<td>71%</td>
</tr>
<tr>
<td>claude-3-haiku</td>
<td>756</td>
<td>91%</td>
<td>91%</td>
<td>69%</td>
</tr>
<tr>
<td>llama-3.2-3b</td>
<td>22</td>
<td>64%</td>
<td>64%</td>
<td>55%</td>
</tr>
<tr>
<td>llama-3.2-1b</td>
<td>909</td>
<td>45%</td>
<td>44%</td>
<td>16%</td>
</tr>
<tr>
<td>qwen2.5-coder-32b</td>
<td>271</td>
<td>27%</td>
<td>27%</td>
<td>0%</td>
</tr>
</tbody></table>
<h2 id="by-context-level">By Context Level</h2>
<p>How much documentation context helps models learn Faber.</p>
<table>
<thead>
<tr>
<th>Context</th>
<th>Tests</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody><tr>
<td>examples-only</td>
<td>2520</td>
<td>60%</td>
</tr>
<tr>
<td>minimal</td>
<td>2816</td>
<td>78%</td>
</tr>
<tr>
<td>basic</td>
<td>2329</td>
<td>79%</td>
</tr>
<tr>
<td>complete</td>
<td>2352</td>
<td>79%</td>
</tr>
</tbody></table>
<h2 id="by-n-shot-learning-curve">By N-shot (Learning Curve)</h2>
<p>Effect of few-shot examples on accuracy.</p>
<table>
<thead>
<tr>
<th>Examples</th>
<th>Tests</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody><tr>
<td>0-shot</td>
<td>2589</td>
<td>66%</td>
</tr>
<tr>
<td>1-shot</td>
<td>2455</td>
<td>70%</td>
</tr>
<tr>
<td>3-shot</td>
<td>2747</td>
<td>79%</td>
</tr>
<tr>
<td>10-shot</td>
<td>2226</td>
<td>80%</td>
</tr>
</tbody></table>
<h2 id="error-distribution">Error Distribution</h2>
<p>Where failures occur (among failed trials only).</p>
<table>
<thead>
<tr>
<th>Error Type</th>
<th>Count</th>
<th>% of Failures</th>
</tr>
</thead>
<tbody><tr>
<td>wrong_output</td>
<td>1100</td>
<td>42%</td>
</tr>
<tr>
<td>type_error</td>
<td>1081</td>
<td>41%</td>
</tr>
<tr>
<td>no_response</td>
<td>268</td>
<td>10%</td>
</tr>
<tr>
<td>syntax_error</td>
<td>173</td>
<td>7%</td>
</tr>
<tr>
<td>runtime_error</td>
<td>9</td>
<td>0%</td>
</tr>
</tbody></table>
<h2 id="by-task">By Task</h2>
<table>
<thead>
<tr>
<th>Task</th>
<th>Tests</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody><tr>
<td>faber_to_ts_functio_string</td>
<td>241</td>
<td>95%</td>
</tr>
<tr>
<td>faber_to_ts_arithmetic</td>
<td>237</td>
<td>93%</td>
</tr>
<tr>
<td>faber_to_ts_si_true</td>
<td>241</td>
<td>92%</td>
</tr>
<tr>
<td>faber_to_ts_ex_pro</td>
<td>240</td>
<td>92%</td>
</tr>
<tr>
<td>faber_to_ts_functio</td>
<td>241</td>
<td>91%</td>
</tr>
<tr>
<td>faber_to_ts_string</td>
<td>241</td>
<td>90%</td>
</tr>
<tr>
<td>faber_to_ts_si_false</td>
<td>241</td>
<td>90%</td>
</tr>
<tr>
<td>faber_to_ts_fixum</td>
<td>240</td>
<td>89%</td>
</tr>
<tr>
<td>faber_to_ts_varia</td>
<td>241</td>
<td>89%</td>
</tr>
<tr>
<td>faber_to_ts_dum</td>
<td>239</td>
<td>88%</td>
</tr>
<tr>
<td>predict_const_value</td>
<td>237</td>
<td>87%</td>
</tr>
<tr>
<td>faber_to_ts_boolean</td>
<td>237</td>
<td>84%</td>
</tr>
<tr>
<td>ts_to_faber_const</td>
<td>241</td>
<td>80%</td>
</tr>
<tr>
<td>complete_const_keyword</td>
<td>237</td>
<td>79%</td>
</tr>
<tr>
<td>ts_to_faber_string</td>
<td>241</td>
<td>78%</td>
</tr>
<tr>
<td>ts_to_faber_arithmetic</td>
<td>239</td>
<td>77%</td>
</tr>
<tr>
<td>complete_return_keyword</td>
<td>237</td>
<td>77%</td>
</tr>
<tr>
<td>complete_let_keyword</td>
<td>237</td>
<td>77%</td>
</tr>
<tr>
<td>complete_function_keyword</td>
<td>236</td>
<td>77%</td>
</tr>
<tr>
<td>ts_to_faber_let</td>
<td>240</td>
<td>76%</td>
</tr>
<tr>
<td>predict_simple_output</td>
<td>237</td>
<td>76%</td>
</tr>
<tr>
<td>complete_while_keyword</td>
<td>237</td>
<td>76%</td>
</tr>
<tr>
<td>predict_arithmetic_parens</td>
<td>236</td>
<td>75%</td>
</tr>
<tr>
<td>predict_loop_sum</td>
<td>236</td>
<td>75%</td>
</tr>
<tr>
<td>complete_print_keyword</td>
<td>236</td>
<td>74%</td>
</tr>
<tr>
<td>ts_to_faber_if_false</td>
<td>241</td>
<td>73%</td>
</tr>
<tr>
<td>predict_function_math</td>
<td>237</td>
<td>73%</td>
</tr>
<tr>
<td>ts_to_faber_if_true</td>
<td>241</td>
<td>72%</td>
</tr>
<tr>
<td>ts_to_faber_while</td>
<td>240</td>
<td>72%</td>
</tr>
<tr>
<td>predict_conditional_true</td>
<td>237</td>
<td>72%</td>
</tr>
<tr>
<td>complete_else_keyword</td>
<td>236</td>
<td>72%</td>
</tr>
<tr>
<td>predict_conditional_false</td>
<td>238</td>
<td>71%</td>
</tr>
<tr>
<td>complete_loop_keyword</td>
<td>237</td>
<td>70%</td>
</tr>
<tr>
<td>predict_arithmetic_precedence</td>
<td>237</td>
<td>66%</td>
</tr>
<tr>
<td>predict_loop_output</td>
<td>236</td>
<td>65%</td>
</tr>
<tr>
<td>ts_to_faber_function</td>
<td>241</td>
<td>64%</td>
</tr>
<tr>
<td>ts_to_faber_for_of</td>
<td>241</td>
<td>63%</td>
</tr>
<tr>
<td>predict_function_call</td>
<td>237</td>
<td>61%</td>
</tr>
<tr>
<td>ts_to_faber_function_string</td>
<td>241</td>
<td>55%</td>
</tr>
<tr>
<td>ts_to_faber_boolean</td>
<td>238</td>
<td>51%</td>
</tr>
<tr>
<td>predict_boolean_and</td>
<td>237</td>
<td>10%</td>
</tr>
<tr>
<td>predict_boolean_or</td>
<td>236</td>
<td>6%</td>
</tr>
</tbody></table>
<h2 id="methodology">Methodology</h2>
<ul>
<li><strong>Temperature:</strong> 0.0 (deterministic)</li>
<li><strong>Seed:</strong> 42 (reproducible)</li>
<li><strong>Dialects:</strong> Latin keywords</li>
<li><strong>Context levels:</strong> examples-only, minimal, basic, complete</li>
</ul>
<p>See <a href="https://github.com/ianzepp/faber-trials">faber-trials</a> for raw data and methodology details.</p>

    </main>
  </div>
</body>
</html>
