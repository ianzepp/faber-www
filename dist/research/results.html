<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Research Results - Faber</title>
  <meta name="description" content="LLM learnability research for Faber">
  <link rel="stylesheet" href="/styles.css">
  <link rel="alternate" type="text/plain" href="/llms.txt" title="LLM Site Index">
  <link rel="alternate" type="text/markdown" href="/faber-complete.md" title="Complete Documentation (Markdown)">
</head>
<body>
  <div class="banner">Pre-alpha: Published for early evaluation only</div>
  <div class="container">
    <nav class="site-nav">
  <a href="/" class="nav-home">Faber</a>
  <div class="nav-section">docs</div>
  <a href="/docs/index.html">Documentation</a>
  <a href="/docs/grammar.html">Grammar Reference</a>
  <a href="/docs/examples.html">Examples</a>
  <a href="/docs/examples-full.html">All Examples</a>
  <a href="/docs/fundamenta.html">Fundamenta</a>
  <a href="/docs/typi.html">Typi</a>
  <a href="/docs/operatores.html">Operatores</a>
  <a href="/docs/structurae.html">Structurae</a>
  <a href="/docs/regimen.html">Regimen</a>
  <a href="/docs/functiones.html">Functiones</a>
  <a href="/docs/importa.html">Importa</a>
  <a href="/docs/errores.html">Errores</a>
  <div class="nav-section">research</div>
  <a href="/research/index.html">Research</a>
  <a href="/research/results.html" class="active">Research Results</a>
  <div class="nav-subheadings">
    <a href="/research/results.html#model-comparison-cost-vs-speed-vs-accuracy">Model Comparison: Cost vs Speed vs Accuracy</a>
    <a href="/research/results.html#three-level-grading-breakdown">Three-Level Grading Breakdown</a>
    <a href="/research/results.html#by-context-level">By Context Level</a>
    <a href="/research/results.html#by-n-shot-learning-curve">By N-shot (Learning Curve)</a>
    <a href="/research/results.html#error-distribution">Error Distribution</a>
    <a href="/research/results.html#by-task">By Task</a>
    <a href="/research/results.html#methodology">Methodology</a>
  </div>
  <a href="/research/framework-1.1.html">Framework 1.1 Results</a>
</nav>
    <main class="content">
      <h1 id="research-results">Research Results</h1>
<p>Results from the Faber evaluation harness. Testing whether LLMs can learn Faber syntax from examples.</p>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Value</th>
</tr>
</thead>
<tbody><tr>
<td>Framework version</td>
<td>1.1</td>
</tr>
<tr>
<td>Total evaluations</td>
<td>13,270</td>
</tr>
<tr>
<td>Models tested</td>
<td>15</td>
</tr>
<tr>
<td>Total cost</td>
<td>$12.04</td>
</tr>
<tr>
<td>Total tokens</td>
<td>9.5M in / 563K out</td>
</tr>
<tr>
<td>Total time</td>
<td>18980.8s</td>
</tr>
</tbody></table>
<h2 id="model-comparison-cost-vs-speed-vs-accuracy">Model Comparison: Cost vs Speed vs Accuracy</h2>
<table>
<thead>
<tr>
<th>Model</th>
<th>Accuracy</th>
<th>Avg Latency</th>
<th>Cost</th>
<th>Tokens</th>
</tr>
</thead>
<tbody><tr>
<td>gpt-4o</td>
<td>89%</td>
<td>829ms</td>
<td>$1.94</td>
<td>707K</td>
</tr>
<tr>
<td>qwen3-coder</td>
<td>89%</td>
<td>1.4s</td>
<td>$0.22</td>
<td>926K</td>
</tr>
<tr>
<td>gpt-3.5-turbo</td>
<td>89%</td>
<td>521ms</td>
<td>$0.40</td>
<td>762K</td>
</tr>
<tr>
<td>gpt-5</td>
<td>89%</td>
<td>6.7s</td>
<td>$4.37</td>
<td>584K</td>
</tr>
<tr>
<td>gpt-4o-mini</td>
<td>88%</td>
<td>869ms</td>
<td>$0.10</td>
<td>630K</td>
</tr>
<tr>
<td>claude-3.5-sonnet</td>
<td>88%</td>
<td>1.8s</td>
<td>$2.21</td>
<td>667K</td>
</tr>
<tr>
<td>llama-3.1-70b</td>
<td>86%</td>
<td>1.1s</td>
<td>$0.21</td>
<td>609K</td>
</tr>
<tr>
<td>codestral</td>
<td>86%</td>
<td>541ms</td>
<td>$0.24</td>
<td>737K</td>
</tr>
<tr>
<td>deepseek-v3.1</td>
<td>85%</td>
<td>2.0s</td>
<td>$0.10</td>
<td>617K</td>
</tr>
<tr>
<td>claude-4.5-sonnet</td>
<td>77%</td>
<td>1.5s</td>
<td>$1.74</td>
<td>518K</td>
</tr>
<tr>
<td>mercury-coder</td>
<td>73%</td>
<td>589ms</td>
<td>$0.22</td>
<td>834K</td>
</tr>
<tr>
<td>llama-3.1-8b</td>
<td>73%</td>
<td>915ms</td>
<td>$0.04</td>
<td>717K</td>
</tr>
<tr>
<td>claude-3-haiku</td>
<td>70%</td>
<td>970ms</td>
<td>$0.22</td>
<td>769K</td>
</tr>
<tr>
<td>llama-3.2-1b</td>
<td>15%</td>
<td>486ms</td>
<td>$0.03</td>
<td>778K</td>
</tr>
<tr>
<td>qwen2.5-coder-32b</td>
<td>0%</td>
<td>7.2s</td>
<td>$0.02</td>
<td>253K</td>
</tr>
</tbody></table>
<h2 id="three-level-grading-breakdown">Three-Level Grading Breakdown</h2>
<p><strong>A</strong> = typechecks, <strong>B</strong> = runs without error, <strong>C</strong> = correct output.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Tests</th>
<th>A (Typechecks)</th>
<th>B (Runs)</th>
<th>C (Correct)</th>
</tr>
</thead>
<tbody><tr>
<td>gpt-4o</td>
<td>952</td>
<td>93%</td>
<td>93%</td>
<td>88%</td>
</tr>
<tr>
<td>qwen3-coder</td>
<td>1068</td>
<td>94%</td>
<td>94%</td>
<td>89%</td>
</tr>
<tr>
<td>gpt-3.5-turbo</td>
<td>1166</td>
<td>91%</td>
<td>91%</td>
<td>89%</td>
</tr>
<tr>
<td>gpt-5</td>
<td>672</td>
<td>93%</td>
<td>93%</td>
<td>89%</td>
</tr>
<tr>
<td>gpt-4o-mini</td>
<td>895</td>
<td>93%</td>
<td>93%</td>
<td>88%</td>
</tr>
<tr>
<td>claude-3.5-sonnet</td>
<td>840</td>
<td>95%</td>
<td>95%</td>
<td>88%</td>
</tr>
<tr>
<td>llama-3.1-70b</td>
<td>870</td>
<td>91%</td>
<td>91%</td>
<td>86%</td>
</tr>
<tr>
<td>codestral</td>
<td>964</td>
<td>93%</td>
<td>92%</td>
<td>86%</td>
</tr>
<tr>
<td>deepseek-v3.1</td>
<td>862</td>
<td>95%</td>
<td>94%</td>
<td>85%</td>
</tr>
<tr>
<td>claude-4.5-sonnet</td>
<td>672</td>
<td>93%</td>
<td>93%</td>
<td>77%</td>
</tr>
<tr>
<td>mercury-coder</td>
<td>840</td>
<td>76%</td>
<td>76%</td>
<td>73%</td>
</tr>
<tr>
<td>llama-3.1-8b</td>
<td>1063</td>
<td>90%</td>
<td>90%</td>
<td>73%</td>
</tr>
<tr>
<td>claude-3-haiku</td>
<td>946</td>
<td>92%</td>
<td>92%</td>
<td>70%</td>
</tr>
<tr>
<td>llama-3.2-1b</td>
<td>1138</td>
<td>43%</td>
<td>43%</td>
<td>15%</td>
</tr>
<tr>
<td>qwen2.5-coder-32b</td>
<td>282</td>
<td>29%</td>
<td>29%</td>
<td>0%</td>
</tr>
</tbody></table>
<h2 id="by-context-level">By Context Level</h2>
<p>How much documentation context helps models learn Faber.</p>
<table>
<thead>
<tr>
<th>Context</th>
<th>Tests</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody><tr>
<td>examples-only</td>
<td>2681</td>
<td>61%</td>
</tr>
<tr>
<td>grammar-only</td>
<td>2662</td>
<td>82%</td>
</tr>
<tr>
<td>minimal</td>
<td>2985</td>
<td>77%</td>
</tr>
<tr>
<td>basic</td>
<td>2466</td>
<td>79%</td>
</tr>
<tr>
<td>complete</td>
<td>2476</td>
<td>79%</td>
</tr>
</tbody></table>
<h2 id="by-n-shot-learning-curve">By N-shot (Learning Curve)</h2>
<p>Effect of few-shot examples on accuracy.</p>
<table>
<thead>
<tr>
<th>Examples</th>
<th>Tests</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody><tr>
<td>0-shot</td>
<td>3343</td>
<td>70%</td>
</tr>
<tr>
<td>1-shot</td>
<td>3073</td>
<td>71%</td>
</tr>
<tr>
<td>3-shot</td>
<td>3914</td>
<td>80%</td>
</tr>
<tr>
<td>10-shot</td>
<td>2940</td>
<td>81%</td>
</tr>
</tbody></table>
<h2 id="error-distribution">Error Distribution</h2>
<p>Where failures occur (among failed trials only).</p>
<table>
<thead>
<tr>
<th>Error Type</th>
<th>Count</th>
<th>% of Failures</th>
</tr>
</thead>
<tbody><tr>
<td>type_error</td>
<td>1360</td>
<td>42%</td>
</tr>
<tr>
<td>wrong_output</td>
<td>1345</td>
<td>42%</td>
</tr>
<tr>
<td>no_response</td>
<td>312</td>
<td>10%</td>
</tr>
<tr>
<td>syntax_error</td>
<td>201</td>
<td>6%</td>
</tr>
<tr>
<td>runtime_error</td>
<td>14</td>
<td>0%</td>
</tr>
</tbody></table>
<h2 id="by-task">By Task</h2>
<table>
<thead>
<tr>
<th>Task</th>
<th>Tests</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody><tr>
<td>faber_to_ts_functio_string</td>
<td>305</td>
<td>95%</td>
</tr>
<tr>
<td>faber_to_ts_arithmetic</td>
<td>303</td>
<td>94%</td>
</tr>
<tr>
<td>faber_to_ts_ex_pro</td>
<td>304</td>
<td>93%</td>
</tr>
<tr>
<td>faber_to_ts_si_true</td>
<td>305</td>
<td>92%</td>
</tr>
<tr>
<td>faber_to_ts_functio</td>
<td>305</td>
<td>92%</td>
</tr>
<tr>
<td>complex_ts_to_faber_factorial</td>
<td>12</td>
<td>92%</td>
</tr>
<tr>
<td>complex_ts_to_faber_fibonacci</td>
<td>12</td>
<td>92%</td>
</tr>
<tr>
<td>complex_ts_to_faber_multi_function</td>
<td>12</td>
<td>92%</td>
</tr>
<tr>
<td>complex_ts_to_faber_ternary_chain</td>
<td>12</td>
<td>92%</td>
</tr>
<tr>
<td>complex_ts_to_faber_string_ops</td>
<td>12</td>
<td>92%</td>
</tr>
<tr>
<td>complex_ts_to_faber_early_return</td>
<td>12</td>
<td>92%</td>
</tr>
<tr>
<td>complex_ts_to_faber_accumulator</td>
<td>12</td>
<td>92%</td>
</tr>
<tr>
<td>complex_ts_to_faber_prime_check</td>
<td>12</td>
<td>92%</td>
</tr>
<tr>
<td>faber_to_ts_fixum</td>
<td>304</td>
<td>91%</td>
</tr>
<tr>
<td>faber_to_ts_string</td>
<td>305</td>
<td>91%</td>
</tr>
<tr>
<td>faber_to_ts_si_false</td>
<td>305</td>
<td>91%</td>
</tr>
<tr>
<td>faber_to_ts_varia</td>
<td>305</td>
<td>90%</td>
</tr>
<tr>
<td>faber_to_ts_dum</td>
<td>303</td>
<td>89%</td>
</tr>
<tr>
<td>predict_const_value</td>
<td>303</td>
<td>87%</td>
</tr>
<tr>
<td>faber_to_ts_boolean</td>
<td>303</td>
<td>85%</td>
</tr>
<tr>
<td>ts_to_faber_const</td>
<td>333</td>
<td>84%</td>
</tr>
<tr>
<td>complex_ts_to_faber_if_in_loop</td>
<td>12</td>
<td>83%</td>
</tr>
<tr>
<td>complex_ts_to_faber_typed_params</td>
<td>12</td>
<td>83%</td>
</tr>
<tr>
<td>complex_ts_to_faber_find_max</td>
<td>12</td>
<td>83%</td>
</tr>
<tr>
<td>ts_to_faber_string</td>
<td>332</td>
<td>82%</td>
</tr>
<tr>
<td>ts_to_faber_arithmetic</td>
<td>331</td>
<td>82%</td>
</tr>
<tr>
<td>complete_const_keyword</td>
<td>302</td>
<td>81%</td>
</tr>
<tr>
<td>ts_to_faber_let</td>
<td>331</td>
<td>80%</td>
</tr>
<tr>
<td>complete_return_keyword</td>
<td>302</td>
<td>79%</td>
</tr>
<tr>
<td>complete_let_keyword</td>
<td>302</td>
<td>79%</td>
</tr>
<tr>
<td>ts_to_faber_if_false</td>
<td>332</td>
<td>78%</td>
</tr>
<tr>
<td>complete_function_keyword</td>
<td>301</td>
<td>78%</td>
</tr>
<tr>
<td>complete_while_keyword</td>
<td>301</td>
<td>78%</td>
</tr>
<tr>
<td>ts_to_faber_if_true</td>
<td>332</td>
<td>77%</td>
</tr>
<tr>
<td>predict_simple_output</td>
<td>303</td>
<td>77%</td>
</tr>
<tr>
<td>complete_print_keyword</td>
<td>300</td>
<td>77%</td>
</tr>
<tr>
<td>ts_to_faber_while</td>
<td>331</td>
<td>76%</td>
</tr>
<tr>
<td>predict_function_math</td>
<td>302</td>
<td>76%</td>
</tr>
<tr>
<td>predict_arithmetic_parens</td>
<td>302</td>
<td>75%</td>
</tr>
<tr>
<td>predict_loop_sum</td>
<td>302</td>
<td>75%</td>
</tr>
<tr>
<td>complex_ts_to_faber_fizzbuzz</td>
<td>12</td>
<td>75%</td>
</tr>
<tr>
<td>complete_else_keyword</td>
<td>301</td>
<td>74%</td>
</tr>
<tr>
<td>predict_conditional_true</td>
<td>303</td>
<td>73%</td>
</tr>
<tr>
<td>complete_loop_keyword</td>
<td>302</td>
<td>72%</td>
</tr>
<tr>
<td>predict_conditional_false</td>
<td>304</td>
<td>71%</td>
</tr>
<tr>
<td>ts_to_faber_for_of</td>
<td>332</td>
<td>67%</td>
</tr>
<tr>
<td>predict_arithmetic_precedence</td>
<td>303</td>
<td>67%</td>
</tr>
<tr>
<td>complex_ts_to_faber_array_type</td>
<td>12</td>
<td>67%</td>
</tr>
<tr>
<td>ts_to_faber_function</td>
<td>332</td>
<td>65%</td>
</tr>
<tr>
<td>predict_loop_output</td>
<td>302</td>
<td>65%</td>
</tr>
<tr>
<td>predict_function_call</td>
<td>302</td>
<td>65%</td>
</tr>
<tr>
<td>ts_to_faber_boolean</td>
<td>331</td>
<td>59%</td>
</tr>
<tr>
<td>complex_ts_to_faber_guard_clause</td>
<td>12</td>
<td>58%</td>
</tr>
<tr>
<td>ts_to_faber_function_string</td>
<td>332</td>
<td>57%</td>
</tr>
<tr>
<td>complex_ts_to_faber_loop_in_loop</td>
<td>14</td>
<td>43%</td>
</tr>
<tr>
<td>complex_ts_to_faber_nested_if</td>
<td>14</td>
<td>29%</td>
</tr>
<tr>
<td>predict_boolean_and</td>
<td>302</td>
<td>16%</td>
</tr>
<tr>
<td>predict_boolean_or</td>
<td>301</td>
<td>13%</td>
</tr>
<tr>
<td>complex_ts_to_faber_higher_order</td>
<td>12</td>
<td>0%</td>
</tr>
<tr>
<td>complex_ts_to_faber_gcd</td>
<td>12</td>
<td>0%</td>
</tr>
<tr>
<td>complex_ts_to_faber_binary_search</td>
<td>14</td>
<td>0%</td>
</tr>
</tbody></table>
<h2 id="methodology">Methodology</h2>
<ul>
<li><strong>Temperature:</strong> 0.0 (deterministic)</li>
<li><strong>Seed:</strong> 42 (reproducible)</li>
<li><strong>Dialects:</strong> Latin keywords</li>
<li><strong>Context levels:</strong> examples-only, minimal, basic, complete</li>
</ul>
<p>See <a href="https://github.com/ianzepp/faber-romanus">faber-romanus</a> for raw data and methodology details.</p>

    </main>
  </div>
</body>
</html>
