<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Thesis and Trials - Faber</title>
  <meta name="description" content="LLM learnability research for Faber">
  <link rel="stylesheet" href="/styles.css">
  <link rel="alternate" type="text/plain" href="/llms.txt" title="LLM Site Index">
  <link rel="alternate" type="text/markdown" href="/faber-complete.md" title="Complete Documentation (Markdown)">
</head>
<body>
  <input type="checkbox" id="nav-toggle" class="nav-toggle">
  <header class="header">
    <div class="header-banner">Pre-alpha: Published for early evaluation only</div>
    <div class="header-bar">
      <a href="/" class="header-logo">Faber</a>
      <label for="nav-toggle" class="header-menu" aria-label="Toggle navigation">
        <span></span>
        <span></span>
        <span></span>
      </label>
    </div>
  </header>
  <div class="container">
    <nav class="site-nav">
  <div class="nav-section">compilers</div>
  <a href="/compilers/faber.html">Faber (Reference)</a>
  <a href="/compilers/rivus.html">Rivus (Bootstrap)</a>
  <div class="nav-section">research</div>
  <a href="/research/index.html" class="active">Thesis and Trials</a>
  <div class="nav-subheadings">
    <a href="/research/index.html#hypothesis">Hypothesis</a>
    <a href="/research/index.html#evaluation">Evaluation</a>
    <a href="/research/index.html#trial-results">Trial Results</a>
    <a href="/research/index.html#model-comparison-cost-vs-speed-vs-accuracy">Model Comparison: Cost vs Speed vs Accuracy</a>
    <a href="/research/index.html#three-level-grading-breakdown">Three-Level Grading Breakdown</a>
    <a href="/research/index.html#by-context-level">By Context Level</a>
    <a href="/research/index.html#by-n-shot-learning-curve">By N-shot (Learning Curve)</a>
    <a href="/research/index.html#error-distribution">Error Distribution</a>
    <a href="/research/index.html#by-task">By Task</a>
    <a href="/research/index.html#methodology">Methodology</a>
  </div>
  <a href="/research/framework-1.1.html">Framework 1.1 Results</a>
  <div class="nav-section">docs</div>
  <a href="/docs/index.html">Documentation</a>
  <a href="/docs/grammar.html">Grammar Reference</a>
  <a href="/docs/examples.html">Examples</a>
  <a href="/docs/examples-full.html">All Examples</a>
  <a href="/docs/fundamenta.html">Fundamenta</a>
  <a href="/docs/typi.html">Typi</a>
  <a href="/docs/operatores.html">Operatores</a>
  <a href="/docs/structurae.html">Structurae</a>
  <a href="/docs/regimen.html">Regimen</a>
  <a href="/docs/functiones.html">Functiones</a>
  <a href="/docs/importa.html">Importa</a>
  <a href="/docs/errores.html">Errores</a>
</nav>
    <main class="content">
      <h1 id="thesis-and-trials">Thesis and Trials</h1>
<p>Can LLMs learn Faber effectively? The <a href="https://github.com/ianzepp/faber-romanus">faber-romanus</a> project includes an evaluation harness to test this systematically.</p>
<h2 id="hypothesis">Hypothesis</h2>
<p>Faber&#39;s design choices - Latin vocabulary, regular morphology, consistent syntax - should make it easier for LLMs to learn from few examples compared to languages optimized for human ergonomics.</p>
<h2 id="evaluation">Evaluation</h2>
<p>The trials test:</p>
<ul>
<li><strong>Multiple models</strong> - From GPT-3.5 to Llama 3.2 1B</li>
<li><strong>N-shot learning</strong> - 0, 1, 3, and 10 example configurations</li>
<li><strong>Task types</strong> - Translation, completion, prediction, explanation</li>
<li><strong>Context levels</strong> - From examples-only to complete documentation</li>
</ul>
<h2 id="trial-results">Trial Results</h2>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Value</th>
</tr>
</thead>
<tbody><tr>
<td>Framework version</td>
<td>1.1</td>
</tr>
<tr>
<td>Total evaluations</td>
<td>13,270</td>
</tr>
<tr>
<td>Models tested</td>
<td>15</td>
</tr>
<tr>
<td>Total cost</td>
<td>$12.04</td>
</tr>
<tr>
<td>Total tokens</td>
<td>9.5M in / 563K out</td>
</tr>
<tr>
<td>Total time</td>
<td>18980.8s</td>
</tr>
</tbody></table>
<h2 id="model-comparison-cost-vs-speed-vs-accuracy">Model Comparison: Cost vs Speed vs Accuracy</h2>
<table>
<thead>
<tr>
<th>Model</th>
<th>Accuracy</th>
<th>Avg Latency</th>
<th>Cost</th>
<th>Tokens</th>
</tr>
</thead>
<tbody><tr>
<td>gpt-4o</td>
<td>89%</td>
<td>829ms</td>
<td>$1.94</td>
<td>707K</td>
</tr>
<tr>
<td>qwen3-coder</td>
<td>89%</td>
<td>1.4s</td>
<td>$0.22</td>
<td>926K</td>
</tr>
<tr>
<td>gpt-3.5-turbo</td>
<td>89%</td>
<td>521ms</td>
<td>$0.40</td>
<td>762K</td>
</tr>
<tr>
<td>gpt-5</td>
<td>89%</td>
<td>6.7s</td>
<td>$4.37</td>
<td>584K</td>
</tr>
<tr>
<td>gpt-4o-mini</td>
<td>88%</td>
<td>869ms</td>
<td>$0.10</td>
<td>630K</td>
</tr>
<tr>
<td>claude-3.5-sonnet</td>
<td>88%</td>
<td>1.8s</td>
<td>$2.21</td>
<td>667K</td>
</tr>
<tr>
<td>llama-3.1-70b</td>
<td>86%</td>
<td>1.1s</td>
<td>$0.21</td>
<td>609K</td>
</tr>
<tr>
<td>codestral</td>
<td>86%</td>
<td>541ms</td>
<td>$0.24</td>
<td>737K</td>
</tr>
<tr>
<td>deepseek-v3.1</td>
<td>85%</td>
<td>2.0s</td>
<td>$0.10</td>
<td>617K</td>
</tr>
<tr>
<td>claude-4.5-sonnet</td>
<td>77%</td>
<td>1.5s</td>
<td>$1.74</td>
<td>518K</td>
</tr>
<tr>
<td>mercury-coder</td>
<td>73%</td>
<td>589ms</td>
<td>$0.22</td>
<td>834K</td>
</tr>
<tr>
<td>llama-3.1-8b</td>
<td>73%</td>
<td>915ms</td>
<td>$0.04</td>
<td>717K</td>
</tr>
<tr>
<td>claude-3-haiku</td>
<td>70%</td>
<td>970ms</td>
<td>$0.22</td>
<td>769K</td>
</tr>
<tr>
<td>llama-3.2-1b</td>
<td>15%</td>
<td>486ms</td>
<td>$0.03</td>
<td>778K</td>
</tr>
<tr>
<td>qwen2.5-coder-32b</td>
<td>0%</td>
<td>7.2s</td>
<td>$0.02</td>
<td>253K</td>
</tr>
</tbody></table>
<h2 id="three-level-grading-breakdown">Three-Level Grading Breakdown</h2>
<p><strong>A</strong> = typechecks, <strong>B</strong> = runs without error, <strong>C</strong> = correct output.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Tests</th>
<th>A (Typechecks)</th>
<th>B (Runs)</th>
<th>C (Correct)</th>
</tr>
</thead>
<tbody><tr>
<td>gpt-4o</td>
<td>952</td>
<td>93%</td>
<td>93%</td>
<td>88%</td>
</tr>
<tr>
<td>qwen3-coder</td>
<td>1068</td>
<td>94%</td>
<td>94%</td>
<td>89%</td>
</tr>
<tr>
<td>gpt-3.5-turbo</td>
<td>1166</td>
<td>91%</td>
<td>91%</td>
<td>89%</td>
</tr>
<tr>
<td>gpt-5</td>
<td>672</td>
<td>93%</td>
<td>93%</td>
<td>89%</td>
</tr>
<tr>
<td>gpt-4o-mini</td>
<td>895</td>
<td>93%</td>
<td>93%</td>
<td>88%</td>
</tr>
<tr>
<td>claude-3.5-sonnet</td>
<td>840</td>
<td>95%</td>
<td>95%</td>
<td>88%</td>
</tr>
<tr>
<td>llama-3.1-70b</td>
<td>870</td>
<td>91%</td>
<td>91%</td>
<td>86%</td>
</tr>
<tr>
<td>codestral</td>
<td>964</td>
<td>93%</td>
<td>92%</td>
<td>86%</td>
</tr>
<tr>
<td>deepseek-v3.1</td>
<td>862</td>
<td>95%</td>
<td>94%</td>
<td>85%</td>
</tr>
<tr>
<td>claude-4.5-sonnet</td>
<td>672</td>
<td>93%</td>
<td>93%</td>
<td>77%</td>
</tr>
<tr>
<td>mercury-coder</td>
<td>840</td>
<td>76%</td>
<td>76%</td>
<td>73%</td>
</tr>
<tr>
<td>llama-3.1-8b</td>
<td>1063</td>
<td>90%</td>
<td>90%</td>
<td>73%</td>
</tr>
<tr>
<td>claude-3-haiku</td>
<td>946</td>
<td>92%</td>
<td>92%</td>
<td>70%</td>
</tr>
<tr>
<td>llama-3.2-1b</td>
<td>1138</td>
<td>43%</td>
<td>43%</td>
<td>15%</td>
</tr>
<tr>
<td>qwen2.5-coder-32b</td>
<td>282</td>
<td>29%</td>
<td>29%</td>
<td>0%</td>
</tr>
</tbody></table>
<h2 id="by-context-level">By Context Level</h2>
<p>How much documentation context helps models learn Faber.</p>
<table>
<thead>
<tr>
<th>Context</th>
<th>Tests</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody><tr>
<td>examples-only</td>
<td>2681</td>
<td>61%</td>
</tr>
<tr>
<td>grammar-only</td>
<td>2662</td>
<td>82%</td>
</tr>
<tr>
<td>minimal</td>
<td>2985</td>
<td>77%</td>
</tr>
<tr>
<td>basic</td>
<td>2466</td>
<td>79%</td>
</tr>
<tr>
<td>complete</td>
<td>2476</td>
<td>79%</td>
</tr>
</tbody></table>
<h2 id="by-n-shot-learning-curve">By N-shot (Learning Curve)</h2>
<p>Effect of few-shot examples on accuracy.</p>
<table>
<thead>
<tr>
<th>Examples</th>
<th>Tests</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody><tr>
<td>0-shot</td>
<td>3343</td>
<td>70%</td>
</tr>
<tr>
<td>1-shot</td>
<td>3073</td>
<td>71%</td>
</tr>
<tr>
<td>3-shot</td>
<td>3914</td>
<td>80%</td>
</tr>
<tr>
<td>10-shot</td>
<td>2940</td>
<td>81%</td>
</tr>
</tbody></table>
<h2 id="error-distribution">Error Distribution</h2>
<p>Where failures occur (among failed trials only).</p>
<table>
<thead>
<tr>
<th>Error Type</th>
<th>Count</th>
<th>% of Failures</th>
</tr>
</thead>
<tbody><tr>
<td>type_error</td>
<td>1360</td>
<td>42%</td>
</tr>
<tr>
<td>wrong_output</td>
<td>1345</td>
<td>42%</td>
</tr>
<tr>
<td>no_response</td>
<td>312</td>
<td>10%</td>
</tr>
<tr>
<td>syntax_error</td>
<td>201</td>
<td>6%</td>
</tr>
<tr>
<td>runtime_error</td>
<td>14</td>
<td>0%</td>
</tr>
</tbody></table>
<h2 id="by-task">By Task</h2>
<table>
<thead>
<tr>
<th>Task</th>
<th>Tests</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody><tr>
<td>faber_to_ts_functio_string</td>
<td>305</td>
<td>95%</td>
</tr>
<tr>
<td>faber_to_ts_arithmetic</td>
<td>303</td>
<td>94%</td>
</tr>
<tr>
<td>faber_to_ts_ex_pro</td>
<td>304</td>
<td>93%</td>
</tr>
<tr>
<td>faber_to_ts_si_true</td>
<td>305</td>
<td>92%</td>
</tr>
<tr>
<td>faber_to_ts_functio</td>
<td>305</td>
<td>92%</td>
</tr>
<tr>
<td>complex_ts_to_faber_factorial</td>
<td>12</td>
<td>92%</td>
</tr>
<tr>
<td>complex_ts_to_faber_fibonacci</td>
<td>12</td>
<td>92%</td>
</tr>
<tr>
<td>complex_ts_to_faber_multi_function</td>
<td>12</td>
<td>92%</td>
</tr>
<tr>
<td>complex_ts_to_faber_ternary_chain</td>
<td>12</td>
<td>92%</td>
</tr>
<tr>
<td>complex_ts_to_faber_string_ops</td>
<td>12</td>
<td>92%</td>
</tr>
<tr>
<td>complex_ts_to_faber_early_return</td>
<td>12</td>
<td>92%</td>
</tr>
<tr>
<td>complex_ts_to_faber_accumulator</td>
<td>12</td>
<td>92%</td>
</tr>
<tr>
<td>complex_ts_to_faber_prime_check</td>
<td>12</td>
<td>92%</td>
</tr>
<tr>
<td>faber_to_ts_fixum</td>
<td>304</td>
<td>91%</td>
</tr>
<tr>
<td>faber_to_ts_string</td>
<td>305</td>
<td>91%</td>
</tr>
<tr>
<td>faber_to_ts_si_false</td>
<td>305</td>
<td>91%</td>
</tr>
<tr>
<td>faber_to_ts_varia</td>
<td>305</td>
<td>90%</td>
</tr>
<tr>
<td>faber_to_ts_dum</td>
<td>303</td>
<td>89%</td>
</tr>
<tr>
<td>predict_const_value</td>
<td>303</td>
<td>87%</td>
</tr>
<tr>
<td>faber_to_ts_boolean</td>
<td>303</td>
<td>85%</td>
</tr>
<tr>
<td>ts_to_faber_const</td>
<td>333</td>
<td>84%</td>
</tr>
<tr>
<td>complex_ts_to_faber_if_in_loop</td>
<td>12</td>
<td>83%</td>
</tr>
<tr>
<td>complex_ts_to_faber_typed_params</td>
<td>12</td>
<td>83%</td>
</tr>
<tr>
<td>complex_ts_to_faber_find_max</td>
<td>12</td>
<td>83%</td>
</tr>
<tr>
<td>ts_to_faber_string</td>
<td>332</td>
<td>82%</td>
</tr>
<tr>
<td>ts_to_faber_arithmetic</td>
<td>331</td>
<td>82%</td>
</tr>
<tr>
<td>complete_const_keyword</td>
<td>302</td>
<td>81%</td>
</tr>
<tr>
<td>ts_to_faber_let</td>
<td>331</td>
<td>80%</td>
</tr>
<tr>
<td>complete_return_keyword</td>
<td>302</td>
<td>79%</td>
</tr>
<tr>
<td>complete_let_keyword</td>
<td>302</td>
<td>79%</td>
</tr>
<tr>
<td>ts_to_faber_if_false</td>
<td>332</td>
<td>78%</td>
</tr>
<tr>
<td>complete_function_keyword</td>
<td>301</td>
<td>78%</td>
</tr>
<tr>
<td>complete_while_keyword</td>
<td>301</td>
<td>78%</td>
</tr>
<tr>
<td>ts_to_faber_if_true</td>
<td>332</td>
<td>77%</td>
</tr>
<tr>
<td>predict_simple_output</td>
<td>303</td>
<td>77%</td>
</tr>
<tr>
<td>complete_print_keyword</td>
<td>300</td>
<td>77%</td>
</tr>
<tr>
<td>ts_to_faber_while</td>
<td>331</td>
<td>76%</td>
</tr>
<tr>
<td>predict_function_math</td>
<td>302</td>
<td>76%</td>
</tr>
<tr>
<td>predict_arithmetic_parens</td>
<td>302</td>
<td>75%</td>
</tr>
<tr>
<td>predict_loop_sum</td>
<td>302</td>
<td>75%</td>
</tr>
<tr>
<td>complex_ts_to_faber_fizzbuzz</td>
<td>12</td>
<td>75%</td>
</tr>
<tr>
<td>complete_else_keyword</td>
<td>301</td>
<td>74%</td>
</tr>
<tr>
<td>predict_conditional_true</td>
<td>303</td>
<td>73%</td>
</tr>
<tr>
<td>complete_loop_keyword</td>
<td>302</td>
<td>72%</td>
</tr>
<tr>
<td>predict_conditional_false</td>
<td>304</td>
<td>71%</td>
</tr>
<tr>
<td>ts_to_faber_for_of</td>
<td>332</td>
<td>67%</td>
</tr>
<tr>
<td>predict_arithmetic_precedence</td>
<td>303</td>
<td>67%</td>
</tr>
<tr>
<td>complex_ts_to_faber_array_type</td>
<td>12</td>
<td>67%</td>
</tr>
<tr>
<td>ts_to_faber_function</td>
<td>332</td>
<td>65%</td>
</tr>
<tr>
<td>predict_loop_output</td>
<td>302</td>
<td>65%</td>
</tr>
<tr>
<td>predict_function_call</td>
<td>302</td>
<td>65%</td>
</tr>
<tr>
<td>ts_to_faber_boolean</td>
<td>331</td>
<td>59%</td>
</tr>
<tr>
<td>complex_ts_to_faber_guard_clause</td>
<td>12</td>
<td>58%</td>
</tr>
<tr>
<td>ts_to_faber_function_string</td>
<td>332</td>
<td>57%</td>
</tr>
<tr>
<td>complex_ts_to_faber_loop_in_loop</td>
<td>14</td>
<td>43%</td>
</tr>
<tr>
<td>complex_ts_to_faber_nested_if</td>
<td>14</td>
<td>29%</td>
</tr>
<tr>
<td>predict_boolean_and</td>
<td>302</td>
<td>16%</td>
</tr>
<tr>
<td>predict_boolean_or</td>
<td>301</td>
<td>13%</td>
</tr>
<tr>
<td>complex_ts_to_faber_higher_order</td>
<td>12</td>
<td>0%</td>
</tr>
<tr>
<td>complex_ts_to_faber_gcd</td>
<td>12</td>
<td>0%</td>
</tr>
<tr>
<td>complex_ts_to_faber_binary_search</td>
<td>14</td>
<td>0%</td>
</tr>
</tbody></table>
<h2 id="methodology">Methodology</h2>
<ul>
<li><strong>Temperature:</strong> 0.0 (deterministic)</li>
<li><strong>Seed:</strong> 42 (reproducible)</li>
<li><strong>Dialects:</strong> Latin keywords</li>
<li><strong>Context levels:</strong> examples-only, minimal, basic, complete</li>
</ul>
<p>See <a href="https://github.com/ianzepp/faber-romanus">faber-romanus</a> for raw data and methodology details.</p>

    </main>
  </div>
  <footer class="footer-bar">
    <a href="https://github.com/ianzepp/faber-romanus" target="_blank" rel="noopener">
      <svg viewBox="0 0 16 16" width="18" height="18" fill="currentColor"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"/></svg>
      Visit or star on GitHub!
    </a>
  </footer>
</body>
</html>
